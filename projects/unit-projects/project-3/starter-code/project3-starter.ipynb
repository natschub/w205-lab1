{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3\n",
    "\n",
    "In this project, you will perform a logistic regression on the admissions data we've been working with in projects 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import pylab as pl\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: Remember in your project unit 2 that you found some missing values? We have to drop them for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   admit  gre   gpa  prestige\n",
      "0      0  380  3.61         3\n",
      "1      1  660  3.67         3\n",
      "2      1  800  4.00         1\n",
      "3      1  640  3.19         4\n",
      "4      0  520  2.93         4\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(\"../assets/admissions.csv\")\n",
    "df = df_raw.dropna() \n",
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "admit       0\n",
      "gre         0\n",
      "gpa         0\n",
      "prestige    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print df.isnull().sum()  #No more nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Frequency Tables\n",
    "\n",
    "What is a frequency table? You are basically counting the number of observations on unique columns of a DataFrame.\n",
    "\n",
    "What does that mean? In our case, we have an admit column and a prestige column. Both are categorical variables as you have verified in previous unit projects. For each admit category AND each prestige category, you want to tabulate the number of observations for each combination of categories. For example, your result should look like:\n",
    "\n",
    "\n",
    "| prestige | 1.0 | 2.0 | 3.0 | 4.0 |\n",
    "|----------|-----|-----|-----|-----|\n",
    "| admit    |     |     |     |     |\n",
    "| 0        | 28  | 95  | 93  | 55  |\n",
    "| 1        | 33  | 53  | 28  | 12  |\n",
    "\n",
    "\n",
    "How do you do this? Use the Pandas **crosstab** function. Go the Pandas documentation and find that function. There is an 'index' argument and a 'column' argument. The index argument says \"values to group-by in the rows\". The rows refer to the tabulated table rows (admit in the example above). Because we want admit to be the rows, we can pass in our admit column of our df DataFrame like this: df['admit']. The crosstab function will know to group all the 1 observations together and the 0 observations together. Then, in the column argument to crosstab, you can pass in your prestige data. Crosstab will take the groups of admit = 1 and 0 and further segment those groups by prestige levels and just do a simple count of how many obeservations are in each.\n",
    "\n",
    "Play around with passing in different columns to the crosstab function and see what happens. You can even pass a list of two columns (note the documentation says you can do that if you look at the index/column argument). Can you explain what is going on?\n",
    "#### 1. Let's create a frequency table of our variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# frequency table for prestige and whether or not someone was admitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the index argument of crosstab, pass in prestige and admit columns in a list and then pass in the gpa column of df into the column argument of crosstab. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do that here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Return of dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Create class or dummy variables for prestige \n",
    "\n",
    "You guys know what function to use here right? Call the dummy frame 'dummy_ranks' to work with starter code in latter cells. Also, use the prefix argument and pass in 'prestige' to it.\n",
    "\n",
    "DO NOT DROP ONE OF THE COLUMNS JUST YET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do the dummy thing in here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 When modeling our class variables, how many of the dummy_ranks columns do we need? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Hand calculating odds ratios\n",
    "\n",
    "Develop your intuition about expected outcomes by hand calculating odds ratios.\n",
    "\n",
    "In the cell below, the .ix DataFrame accessor will attempt to use the index LABEL to retrieve elements and if\n",
    "it doesn't find any matching labels, it just uses the position in the frame. In the below code, .ix[:,'prestige_1':] accesses the rows by ':' which means 'grab all rows' and can be used for labels OR positions. The 'prestige_1:' means grab all columns starting at 'prestige_1' onwards. In the case of dummy_ranks, that's all of the columns hence the .ix accessor just grabs the whole DataFrame as is (no subsetting happens here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "handCalc = df[cols_to_keep].join(dummy_ranks.ix[:, 'prestige_1':])\n",
    "print handCalc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.0 Just as you've done, create a crosstab for admit in the rows of the crosstab and prestige_1.0 in the columns of the crosstab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Run crosstab here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Use the cross tab above to calculate the odds of being admitted to grad school if you attended a #1 ranked \n",
    "\n",
    "Recall Odds: Odds represent $$\\frac{P(\\text{something happening})}{P(\\text{something not happening})}$$\n",
    "\n",
    "What the question asks is tocalculate $$\\frac{P(\\text{admit}\\hspace{2mm}|\\hspace{2mm}\\text{attended rank-1 school})}{P(\\text{not admit}\\hspace{2mm}|\\hspace{2mm}\\text{attended rank-1 school})}$$\n",
    "\n",
    "Because we are concerned with probabilities GIVEN that an observation fell into the 'rank-1 school' = 1 bucket, you should take  a look at the column of the crosstab that has 'prestige_1.0' = 1\n",
    "\n",
    "How do you calculate probabilities within columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Now calculate the odds of admission if you did not attend a #1 ranked college"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Calculate the odds ratio (OR). \n",
    "\n",
    "Basically take the number you calculated in 3.1 and 3.2 and divide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Write this finding in a sentence: \n",
    "\n",
    "How do you interpret this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Do the same as above with prestige_4.0 columns.\n",
    "\n",
    "Print the crosstab below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Calculate the Odds Ratio of admission given attended prestige_4.0 school to not attended prestige_4.0 school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Write this finding in a sentence. Also, compare to the prestige_1.0 odds ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Analysis\n",
    "\n",
    "Use statsmodels and Scikit Learn to do a Logistic Regression of all the co-variates.\n",
    "\n",
    "Background Info:\n",
    "\n",
    "Remember Starter Code for Lesson 6? There we used a package called statsmodels to do a regression analysis. You can also use Statsmodels to do a Logistic Regression. But you can also do a Logistic Regression in Scikit Learn. Why do we need both?\n",
    "\n",
    "1. Statsmodels - Great when you want quick and easy statistical interpretation of your model. The P-values are output as part of the fitting process.\n",
    "\n",
    "2. Scikit Learn - Great when you want full-blown modelling and Machine Learning and Optimization tools. Think GridSearchCV or Cross Validation\n",
    "\n",
    "It's actually a great idea to use both of them as you are doing an analysis. You can gain some insight into which variables are statistically significant and hence worthwhile keeping in a model by using statsmodels. If your variables are statistically significant, then you have more confidence that your model will generalize and a relationship wasn't found by fitting random noise in your particular sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   admit  gre   gpa  prestige_2.0  prestige_3.0  prestige_4.0\n",
      "0      0  380  3.61             0             1             0\n",
      "1      1  660  3.67             0             1             0\n",
      "2      1  800  4.00             0             0             0\n",
      "3      1  640  3.19             0             0             1\n",
      "4      0  520  2.93             0             0             1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a clean data frame for the regression\n",
    "cols_to_keep = ['admit', 'gre', 'gpa']\n",
    "data = df[cols_to_keep].join(dummy_ranks.ix[:, 'prestige_2':])\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to add a constant term for our Logistic Regression. The statsmodels function we're going to be using requires that intercepts/constants are specified explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually add the intercept\n",
    "data['intercept'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about why adding an intercept column of all 1's actually represents an intercept term in the regression?\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Set the covariates to a variable called train_cols\n",
    "\n",
    "This is basically your X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fit the model\n",
    "\n",
    "The documentation is a little bit tricky. It can be found here: http://statsmodels.sourceforge.net/0.6.0/generated/statsmodels.discrete.discrete_model.Logit.html\n",
    "\n",
    "By the way, notice how we imported statsmodels.api in the statement \"import statsmodels.api as sm\" at the top of the project. What does that mean? It clearly looks different from the module namespace for Logit (statsmodels.discrete.discrete_model.Logit) in the Logit docs. This helps explain what is going on: http://statsmodels.sourceforge.net/devel/importpaths.html\n",
    "\n",
    "Basically, the fit process involves 2 steps. If you have problems here, please Slack me ASAP\n",
    "\n",
    "1. Create a Logit object passing in the correct arguments. \"endog\" is the dependent variable according to the documentation while \"exog\" must be the independent variables.\n",
    "\n",
    "2. Fit the model. Does the fit model need any arguments? Check out the documentation for the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Print the summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please interpret the results. Be as thorough as you can. In your answer, talk about things such as:\n",
    "\n",
    "1. What are the significant predictors?\n",
    "\n",
    "2. What do the coefficients mean?\n",
    "\n",
    "3. How do you explain the dummy-variable coefficients in particular. Do the numbers make sense?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Calculate the change in odds of the coeffiencents and their 95% CI intervals\n",
    "\n",
    "Recall that the Logistic Regression model is the following:\n",
    "\n",
    "$$P(y = 1 | X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p)}}$$ \n",
    "\n",
    "Rearranging this equation gives us the odds (shortening $P(y=1|X)$ to P):\n",
    "\n",
    "$$e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p} = \\frac{P}{1-P}$$\n",
    "\n",
    "Now, what happens to the odds when we increase $X_1$ by one unit:\n",
    "\n",
    "$$e^{\\beta_0 + \\beta_1(X_1 + 1) + ... + \\beta_pX_p} = e^{\\beta_1}e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p} = \\frac{P}{1-P}e^{\\beta_1}$$\n",
    "\n",
    "The takeaway is that when we increase $X_i$ by one unit, the *odds* change by a factor of $e^{\\beta_i}$\n",
    "\n",
    "1. Calculate the change in odds associated by a one-unit increase in **each** of the variables\n",
    "\n",
    "2. From the model, extract the confidence intervals associated with **each** of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do part 1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do part 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the lower end of the confidence interval mean? The upper end? Please interpret\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Interpret the odds change associated with Prestige_2.0\n",
    "\n",
    "That is, using the numbers you just found above, what do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Interpret the odds change of GPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Model Fit using Scikit Learn\n",
    "\n",
    "We started this in class on Monday 9/19. You shouldn't have much problem with this because we've done it many times now. If you run into trouble please Slack me ASAP.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fit here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Print out the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Compare to your Statsmodels Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Predicted probablities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** -- Think of the below as a way to generate a test set from the data.\n",
    "\n",
    "As a way of evaluating our classifier, we're going to recreate the dataset with every logical combination of input values. This will allow us to see how the predicted probability of admission increases/decreases across different variables. First we're going to generate the combinations using a helper function called cartesian (above).\n",
    "\n",
    "We're going to use np.linspace to create a range of values for \"gre\" and \"gpa\". This creates a range of linearly spaced values from a specified min and maximum value--in our case just the min/max observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cartesian(arrays, out=None):\n",
    "    \"\"\"\n",
    "    Generate a cartesian product of input arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    arrays : list of array-like\n",
    "        1-D arrays to form the cartesian product of.\n",
    "    out : ndarray\n",
    "        Array to place the cartesian product in.\n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray\n",
    "        2-D array of shape (M, len(arrays)) containing cartesian products\n",
    "        formed of input arrays.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> cartesian(([1, 2, 3], [4, 5], [6, 7]))\n",
    "    array([[1, 4, 6],\n",
    "           [1, 4, 7],\n",
    "           [1, 5, 6],\n",
    "           [1, 5, 7],\n",
    "           [2, 4, 6],\n",
    "           [2, 4, 7],\n",
    "           [2, 5, 6],\n",
    "           [2, 5, 7],\n",
    "           [3, 4, 6],\n",
    "           [3, 4, 7],\n",
    "           [3, 5, 6],\n",
    "           [3, 5, 7]])\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [np.asarray(x) for x in arrays]\n",
    "    dtype = arrays[0].dtype\n",
    "\n",
    "    n = np.prod([x.size for x in arrays])\n",
    "    if out is None:\n",
    "        out = np.zeros([n, len(arrays)], dtype=dtype)\n",
    "\n",
    "    m = n / arrays[0].size\n",
    "    out[:,0] = np.repeat(arrays[0], m)\n",
    "    if arrays[1:]:\n",
    "        cartesian(arrays[1:], out=out[0:m,1:])\n",
    "        for j in xrange(1, arrays[0].size):\n",
    "            out[j*m:(j+1)*m,1:] = out[0:m,1:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 220.          284.44444444  348.88888889  413.33333333  477.77777778\n",
      "  542.22222222  606.66666667  671.11111111  735.55555556  800.        ]\n",
      "[ 2.26        2.45333333  2.64666667  2.84        3.03333333  3.22666667\n",
      "  3.42        3.61333333  3.80666667  4.        ]\n"
     ]
    }
   ],
   "source": [
    "# instead of generating all possible values of GRE and GPA, we're going\n",
    "# to use an evenly spaced range of 10 values from the min to the max \n",
    "gres = np.linspace(data['gre'].min(), data['gre'].max(), 10)\n",
    "print gres\n",
    "# array([ 220.        ,  284.44444444,  348.88888889,  413.33333333,\n",
    "#         477.77777778,  542.22222222,  606.66666667,  671.11111111,\n",
    "#         735.55555556,  800.        ])\n",
    "gpas = np.linspace(data['gpa'].min(), data['gpa'].max(), 10)\n",
    "print gpas\n",
    "# array([ 2.26      ,  2.45333333,  2.64666667,  2.84      ,  3.03333333,\n",
    "#         3.22666667,  3.42      ,  3.61333333,  3.80666667,  4.        ])\n",
    "\n",
    "\n",
    "# enumerate all possibilities\n",
    "combos = pd.DataFrame(cartesian([gres, gpas, [1, 2, 3, 4], [1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Recreate the dummy variables\n",
    "\n",
    "Create the dummy column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# recreate the dummy variables\n",
    "\n",
    "# keep only what we need for making predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Make probability predictions on the enumerated dataset\n",
    "\n",
    "Do this on your **Statsmodels** fit. Use the predict function: http://statsmodels.sourceforge.net/0.6.0/generated/statsmodels.discrete.discrete_model.Logit.predict.html#statsmodels.discrete.discrete_model.Logit.predict\n",
    "\n",
    "Note the last argument is called \"linear\" and it is optional with a default set to False. The docs say \"Else, returns the value of the cdf at the linear predictor.\" The value of the CDF is basically the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Interpret findings for the last 4 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Plot the probability of being admitted into graduate school, stratified by GPA and GRE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
