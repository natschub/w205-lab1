{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Spam Filter\n",
    "\n",
    "Introduction: Naive Bayes is one of the easiest classification algorithms to understand. It uses Bayes Rule to model the probability of a class label given some inputs.\n",
    "\n",
    "### Probability Review: Understanding Naive Bayes requires remembering basic probability relationships.\n",
    "\n",
    "**Conditional Probability**: Take two events A and B where B is known to have occured. Then, the conditional probability of A given B is defined:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A,B)}{P(B)}$$\n",
    "\n",
    "**Bayes Rule**: Does the following:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A,B)}{P(B)} = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Without the intermediate equal sign:\n",
    "$$P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$$\n",
    "\n",
    "Why is that useful? If you need to calculate $P(A|B)$ but it's easier to computer $P(B|A)$ then it is a good idea to use Bayes Rule. For example, $P(\\text{spam} \\hspace{2mm}| \\hspace{2mm}\\text{'enlargement' in email})$ is not clear how to calculate directly. But, $P(\\text{'enlargement' in email} \\hspace{2mm}| \\hspace{2mm}\\text{spam})$ is a little easier. Perhaps we count how many times enlargement is seen in spam emails...\n",
    "\n",
    "**Independence**: This is one of the most powerful concepts in probability. It says that if two events A and B are independent then \n",
    "\n",
    "$$P(A,B) = P(A)P(B)$$\n",
    "\n",
    "We can combine conditional probability and Independence together:\n",
    "\n",
    "$$P(A,B|C) = P(A|C)P(B|C)$$\n",
    "\n",
    "This expression is interpreted as \"Given I know C happened, what is my probability of observing A AND B?\" Since A and B are independent, that is equal to the probability of observing A given C occured times the probability of observing B given C occured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Spam Filter Setup\n",
    "\n",
    "An email is a collection of words: \n",
    "                                   $$email_1 = \\{Hi,Bob,It,was,a,pleasure,meeting,you,today\\}$$\n",
    "                                   $$email_2 = \\{One,Day,Sale,Buy,Now\\}$$\n",
    "                                   \n",
    "In the email filtering problem, we are interested in calculating $P(spam\\hspace{2mm}|\\hspace{2mm} email_1)$. A sensible approach would be to classify as SPAM if that probability > $P(ham\\hspace{2mm}|\\hspace{2mm} email_1)$\n",
    "### Exercise 1: \n",
    "1. Use Bayes Rule please rewrite $P(spam\\hspace{2mm}|\\hspace{2mm} \\{Hi,Bob,It,was,a,pleasure,meeting,you,today\\})$\n",
    "2. Make the NAIVE assumption that all words in an email are independent of one another. What does Bayes Rule simplify to?\n",
    "\n",
    "\n",
    "### Exercise 2:\n",
    "Before we keep going, let's take the first steps of analysis. There are one file for you to work with. \n",
    "Each line contains something like 'hi bob it was a pleasure - 0'\n",
    "1. The text is corresponding to an email. Read the text lines into a dictionary of lists. Key = 'email_1' <-> Value = ['Hi','Bob',...]\n",
    "2. At the end of each line, there is a ' - 0' or ' - 1'. That denotes whether the line is SPAM = 1 or HAM = 0. Read that into a pandas Series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do Exercise 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: \n",
    "Let's predict spam or ham for a new email. Consider the email \n",
    "\n",
    "$$email_{test} = \\{our,boss,put,the,sale,meet,time,on,the,schedule\\}$$\n",
    "\n",
    "1. We want to estimate $P(spam\\hspace{2mm}|\\hspace{2mm} \\{our,boss,put,the,sale,meet,time,on,the,schedule\\})$. Use the Conditional Bayes Rule to flip the probabilities.\n",
    "2. Write a function to calculate the prior probability $P(class)$. Pass class into the function as a string i.e. 'spam'.\n",
    "   Think about how to estimate this probability from your training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a prior function defined, we need to define a likelihood probability $P(\\text{word} \\hspace{2mm}|\\hspace{2mm} \\text{class})$. That is, the likelihood of a word given a class label.\n",
    "1. Write a function that takes a word and a class label and returns the estimated conditional probability. Think about how to calculate that probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do with the denominator of the Bayes Expression? \n",
    "\n",
    "$$P(\\{our,boss,put,the,sale,meet,time,on,the,schedule\\})$$\n",
    "\n",
    "For the purposes of classification, we don't care to calculate it much because both conditional probabilities $P(spam\\hspace{2mm}|\\hspace{2mm} email)$ and $P(ham\\hspace{2mm}|\\hspace{2mm} email)$ will have that same factor in the denominator and all we care about is a comparison between the two to know which is larger, so we can ignore the denominator.\n",
    "\n",
    "Now that you have your prior class probability and likelihood functions, inside a PREDICT function, calculate the numerators and compare to each other. Return the class label corresponding to max. Sometimes these quantities are called the score functions:\n",
    "\n",
    "1. $P(our \\hspace{2mm} |\\hspace{2mm} spam)P(boss \\hspace{2mm} |\\hspace{2mm} spam)P(put \\hspace{2mm} |\\hspace{2mm} spam)P(the \\hspace{2mm} |\\hspace{2mm} spam)P(sale \\hspace{2mm} |\\hspace{2mm} spam)P(meet \\hspace{2mm} |\\hspace{2mm} spam)P(time \\hspace{2mm} |\\hspace{2mm} spam)P(on \\hspace{2mm} |\\hspace{2mm} spam)P(the \\hspace{2mm} |\\hspace{2mm} spam)P(schedule \\hspace{2mm} |\\hspace{2mm} spam)*P(spam)$\n",
    "\n",
    "2. $P(our \\hspace{2mm} |\\hspace{2mm} ham)P(boss \\hspace{2mm} |\\hspace{2mm} ham)P(put \\hspace{2mm} |\\hspace{2mm} ham)P(the \\hspace{2mm} |\\hspace{2mm} ham)P(sale \\hspace{2mm} |\\hspace{2mm} ham)P(meet \\hspace{2mm} |\\hspace{2mm} ham)P(time \\hspace{2mm} |\\hspace{2mm} ham)P(on \\hspace{2mm} |\\hspace{2mm} ham)P(the \\hspace{2mm} |\\hspace{2mm} ham)P(schedule \\hspace{2mm} |\\hspace{2mm} ham)*P(ham)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Something looks weird. Why are all the predictions ham (or spam depending on your implementation details)? To troubleshoot, let's look at the actual Bayes numerators.\n",
    "\n",
    "1. In your predict function, place a print statement that lets you view the scores for each class\n",
    "2. Why do you think you get the value 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Cont.:\n",
    "\n",
    "How do we fix this? Hint: Apply a 1:1 mathematical function to your score function. What's a good function to use?\n",
    "\n",
    "1. Do that now. Calculate the function of the scores symbolically from the above expressions (end of exercise 3). Convince me that this will solve your prediction issue.\n",
    "\n",
    "2. Now armed with that expression, update your predict function to calculate that instead. Don't overwrite what you have, just copy the code and call the function something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do that here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.:\n",
    "\n",
    "What happens if we get an email with a word that doesn't exist at all in our training set? Maybe...\n",
    "\n",
    "$$email_{new} = \\{There,is,a,fire,at,the,office\\}$$\n",
    "\n",
    "Note that fire does not exist in your training set.\n",
    "\n",
    "1. Use your predict function to classify this new email. What happens?\n",
    "\n",
    "2. Mathematically, what is the fundamental problem?\n",
    "\n",
    "3. Can you think of a way to fix this? Hint: Perhaps there's a way to modify how we calculate our likelihoods i.e. $P(word \\hspace{2mm} |\\hspace{2mm} class)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5. Cont. (Laplace Smoothing):\n",
    "\n",
    "One way that is very common is to use Laplace Smoothing. This technique calculates\n",
    "\n",
    "$$P(\\text{word} \\hspace{2mm}|\\hspace{2mm} \\text{class}) = \\frac{N_{word-in-class} + 1}{N_{total-words-in-class} + |V|}$$ \n",
    "\n",
    "where $|V|$ is defined as the number of unique words in the class. Sometimes this is called the dictionary size of the class in the text-classification context.\n",
    "\n",
    "1. Is that a valid way of estimating the probability? That is, is it a probability? Convince me. **Hint**: does the conditional probability add up to 1 when summed over all words in the class dictionary?\n",
    "\n",
    "2. Fix your predict function again to take Laplace Smoothing into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Exercise:\n",
    "\n",
    "Can you figure out how to do text classification using Logist Regression or perhaps KNN? Is it do-able?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
