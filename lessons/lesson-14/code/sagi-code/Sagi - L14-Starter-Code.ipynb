{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "import codecs\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "# spacy is used for pre-processing and traditional NLP\n",
    "import spacy\n",
    "from spacy.en import English\n",
    "\n",
    "# Gensim is used for LDA and word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>urlid</th>\n",
       "      <th>boilerplate</th>\n",
       "      <th>alchemy_category</th>\n",
       "      <th>alchemy_category_score</th>\n",
       "      <th>avglinksize</th>\n",
       "      <th>commonlinkratio_1</th>\n",
       "      <th>commonlinkratio_2</th>\n",
       "      <th>commonlinkratio_3</th>\n",
       "      <th>commonlinkratio_4</th>\n",
       "      <th>...</th>\n",
       "      <th>linkwordscore</th>\n",
       "      <th>news_front_page</th>\n",
       "      <th>non_markup_alphanum_characters</th>\n",
       "      <th>numberOfLinks</th>\n",
       "      <th>numwords_in_url</th>\n",
       "      <th>parametrizedLinkRatio</th>\n",
       "      <th>spelling_errors_ratio</th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bloomberg.com/news/2010-12-23/ibm-p...</td>\n",
       "      <td>4042</td>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "      <td>business</td>\n",
       "      <td>0.789131</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>5424</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.079130</td>\n",
       "      <td>0</td>\n",
       "      <td>IBM Sees Holographic Calls Air Breathing Batte...</td>\n",
       "      <td>A sign stands outside the International Busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.popsci.com/technology/article/2012-...</td>\n",
       "      <td>8471</td>\n",
       "      <td>{\"title\":\"The Fully Electronic Futuristic Star...</td>\n",
       "      <td>recreation</td>\n",
       "      <td>0.574147</td>\n",
       "      <td>3.677966</td>\n",
       "      <td>0.508021</td>\n",
       "      <td>0.288770</td>\n",
       "      <td>0.213904</td>\n",
       "      <td>0.144385</td>\n",
       "      <td>...</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4973</td>\n",
       "      <td>187</td>\n",
       "      <td>9</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.125448</td>\n",
       "      <td>1</td>\n",
       "      <td>The Fully Electronic Futuristic Starting Gun T...</td>\n",
       "      <td>And that can be carried on a plane without the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.menshealth.com/health/flu-fighting-...</td>\n",
       "      <td>1164</td>\n",
       "      <td>{\"title\":\"Fruits that Fight the Flu fruits tha...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>2.382883</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.321705</td>\n",
       "      <td>0.120155</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2240</td>\n",
       "      <td>258</td>\n",
       "      <td>11</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.057613</td>\n",
       "      <td>1</td>\n",
       "      <td>Fruits that Fight the Flu fruits that fight th...</td>\n",
       "      <td>Apples The most popular source of antioxidants...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.dumblittleman.com/2007/12/10-foolpr...</td>\n",
       "      <td>6684</td>\n",
       "      <td>{\"title\":\"10 Foolproof Tips for Better Sleep \"...</td>\n",
       "      <td>health</td>\n",
       "      <td>0.801248</td>\n",
       "      <td>1.543103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2737</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.100858</td>\n",
       "      <td>1</td>\n",
       "      <td>10 Foolproof Tips for Better Sleep</td>\n",
       "      <td>There was a period in my life when I had a lot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://bleacherreport.com/articles/1205138-the...</td>\n",
       "      <td>9006</td>\n",
       "      <td>{\"title\":\"The 50 Coolest Jerseys You Didn t Kn...</td>\n",
       "      <td>sports</td>\n",
       "      <td>0.719157</td>\n",
       "      <td>2.676471</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.043210</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>12032</td>\n",
       "      <td>162</td>\n",
       "      <td>10</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>0</td>\n",
       "      <td>The 50 Coolest Jerseys You Didn t Know Existed...</td>\n",
       "      <td>Jersey sales is a curious business Whether you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  urlid  \\\n",
       "0  http://www.bloomberg.com/news/2010-12-23/ibm-p...   4042   \n",
       "1  http://www.popsci.com/technology/article/2012-...   8471   \n",
       "2  http://www.menshealth.com/health/flu-fighting-...   1164   \n",
       "3  http://www.dumblittleman.com/2007/12/10-foolpr...   6684   \n",
       "4  http://bleacherreport.com/articles/1205138-the...   9006   \n",
       "\n",
       "                                         boilerplate alchemy_category  \\\n",
       "0  {\"title\":\"IBM Sees Holographic Calls Air Breat...         business   \n",
       "1  {\"title\":\"The Fully Electronic Futuristic Star...       recreation   \n",
       "2  {\"title\":\"Fruits that Fight the Flu fruits tha...           health   \n",
       "3  {\"title\":\"10 Foolproof Tips for Better Sleep \"...           health   \n",
       "4  {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...           sports   \n",
       "\n",
       "  alchemy_category_score  avglinksize  commonlinkratio_1  commonlinkratio_2  \\\n",
       "0               0.789131     2.055556           0.676471           0.205882   \n",
       "1               0.574147     3.677966           0.508021           0.288770   \n",
       "2               0.996526     2.382883           0.562016           0.321705   \n",
       "3               0.801248     1.543103           0.400000           0.100000   \n",
       "4               0.719157     2.676471           0.500000           0.222222   \n",
       "\n",
       "   commonlinkratio_3  commonlinkratio_4  \\\n",
       "0           0.047059           0.023529   \n",
       "1           0.213904           0.144385   \n",
       "2           0.120155           0.042636   \n",
       "3           0.016667           0.000000   \n",
       "4           0.123457           0.043210   \n",
       "\n",
       "                         ...                          linkwordscore  \\\n",
       "0                        ...                                     24   \n",
       "1                        ...                                     40   \n",
       "2                        ...                                     55   \n",
       "3                        ...                                     24   \n",
       "4                        ...                                     14   \n",
       "\n",
       "   news_front_page  non_markup_alphanum_characters  numberOfLinks  \\\n",
       "0                0                            5424            170   \n",
       "1                0                            4973            187   \n",
       "2                0                            2240            258   \n",
       "3                0                            2737            120   \n",
       "4                0                           12032            162   \n",
       "\n",
       "   numwords_in_url  parametrizedLinkRatio  spelling_errors_ratio label  \\\n",
       "0                8               0.152941               0.079130     0   \n",
       "1                9               0.181818               0.125448     1   \n",
       "2               11               0.166667               0.057613     1   \n",
       "3                5               0.041667               0.100858     1   \n",
       "4               10               0.098765               0.082569     0   \n",
       "\n",
       "                                               title  \\\n",
       "0  IBM Sees Holographic Calls Air Breathing Batte...   \n",
       "1  The Fully Electronic Futuristic Starting Gun T...   \n",
       "2  Fruits that Fight the Flu fruits that fight th...   \n",
       "3                10 Foolproof Tips for Better Sleep    \n",
       "4  The 50 Coolest Jerseys You Didn t Know Existed...   \n",
       "\n",
       "                                                body  \n",
       "0  A sign stands outside the International Busine...  \n",
       "1  And that can be carried on a plane without the...  \n",
       "2  Apples The most popular source of antioxidants...  \n",
       "3  There was a period in my life when I had a lot...  \n",
       "4  Jersey sales is a curious business Whether you...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode Handling\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = pd.read_csv(\"../../assets/dataset/stumbleupon.tsv\", sep='\\t',\n",
    "                  encoding=\"utf-8\")\n",
    "data['title'] = data.boilerplate.map(lambda x: json.loads(x).get('title', ''))\n",
    "data['body'] = data.boilerplate.map(lambda x: json.loads(x).get('body', ''))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = data['body'].dropna()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary = 'False',stop_words = 'english',min_df=3)\n",
    "\n",
    "docs = cv.fit_transform(body)\n",
    "\n",
    "id2word = dict(enumerate(cv.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "\n",
    "#What does this do?\n",
    "#Convert our sparse matrix into a gensim corpus stream\n",
    "corpus = Sparse2Corpus(docs, documents_columns = False)\n",
    "\n",
    "#We need to pass in the dictionary so that way, we can map our integers that the LDA model works with back to\n",
    "#the original vocab word.\n",
    "lda_model = LdaModel(corpus = corpus, id2word = id2word,num_topics = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.matutils.Sparse2Corpus"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "(0, u'0.006*illustrations + 0.006*copied + 0.005*sinking + 0.005*paradise + 0.005*sculptures')\n",
      "Topic: 1\n",
      "(1, u'0.009*funny + 0.008*com + 0.006*pictures + 0.006*video + 0.006*http')\n",
      "Topic: 2\n",
      "(2, u'0.014*blessed + 0.013*counteract + 0.009*compromise + 0.007*certificate + 0.007*career')\n",
      "Topic: 3\n",
      "(3, u'0.006*passing + 0.005*player + 0.005*nfl + 0.005*quarterback + 0.005*defensive')\n",
      "Topic: 4\n",
      "(4, u'0.011*scrumptious + 0.004*recipe + 0.004*recipes + 0.004*discarding + 0.004*make')\n",
      "Topic: 5\n",
      "(5, u'0.006*wearable + 0.005*videos + 0.005*pics + 0.004*art + 0.004*decker')\n",
      "Topic: 6\n",
      "(6, u'0.003*like + 0.003*cattle + 0.002*consists + 0.002*just + 0.002*hood')\n",
      "Topic: 7\n",
      "(7, u'0.006*bowling + 0.005*harris + 0.004*dow + 0.004*men + 0.004*harry')\n",
      "Topic: 8\n",
      "(8, u'0.008*01 + 0.007*console + 0.007*2500 + 0.007*generator + 0.006*buyer')\n",
      "Topic: 9\n",
      "(9, u'0.005*just + 0.004*browns + 0.003*like + 0.003*hash + 0.003*peach')\n",
      "Topic: 10\n",
      "(10, u'0.005*vogue + 0.005*oregon + 0.004*heritage + 0.004*searches + 0.004*tribute')\n",
      "Topic: 11\n",
      "(11, u'0.019*clutch + 0.012*async + 0.012*slather + 0.011*wid + 0.011*widget')\n",
      "Topic: 12\n",
      "(12, u'0.003*new + 0.002*just + 0.002*2011 + 0.002*time + 0.002*like')\n",
      "Topic: 13\n",
      "(13, u'0.003*new + 0.003*good + 0.003*people + 0.003*life + 0.003*like')\n",
      "Topic: 14\n",
      "(14, u'0.015*flats + 0.007*howto + 0.007*les + 0.007*sur + 0.006*matrix')\n",
      "Topic: 15\n",
      "(15, u'0.010*junkie + 0.008*scotland + 0.008*scottish + 0.007*confirmation + 0.007*submit')\n",
      "Topic: 16\n",
      "(16, u'0.012*05 + 0.011*indie + 0.010*01 + 0.010*header + 0.010*lookbook')\n",
      "Topic: 17\n",
      "(17, u'0.005*new + 0.003*2012 + 0.003*world + 0.003*online + 0.003*news')\n",
      "Topic: 18\n",
      "(18, u'0.007*like + 0.004*max + 0.004*fuck + 0.004*block + 0.004*hidden')\n",
      "Topic: 19\n",
      "(19, u'0.013*enticing + 0.011*dj + 0.008*suspiciously + 0.007*rsquo + 0.006*cuz')\n",
      "Topic: 20\n",
      "(20, u'0.008*april + 0.008*september + 0.008*june + 0.008*october + 0.007*february')\n",
      "Topic: 21\n",
      "(21, u'0.008*sidebar + 0.007*802 + 0.007*motive + 0.007*penny + 0.006*petty')\n",
      "Topic: 22\n",
      "(22, u'0.005*asks + 0.004*popup + 0.003*homework + 0.003*sheriff + 0.003*tower')\n",
      "Topic: 23\n",
      "(23, u'0.021*delectable + 0.013*dumplings + 0.011*nyc + 0.009*counterpart + 0.009*snowboarding')\n",
      "Topic: 24\n",
      "(24, u'0.010*photos + 0.010*viable + 0.009*blogs + 0.009*food + 0.008*watering')\n",
      "Topic: 25\n",
      "(25, u'0.010*sauce1 + 0.007*avocado + 0.005*pepper + 0.005*cheese + 0.005*recipe')\n",
      "Topic: 26\n",
      "(26, u'0.006*recipe + 0.006*salt + 0.005*minutes + 0.005*cup + 0.004*add')\n",
      "Topic: 27\n",
      "(27, u'0.015*password + 0.011*register + 0.011*address + 0.010*notification + 0.010*animation')\n",
      "Topic: 28\n",
      "(28, u'0.008*restrict + 0.008*shoulders + 0.007*boredom + 0.006*tips + 0.006*interval')\n",
      "Topic: 29\n",
      "(29, u'0.005*just + 0.005*like + 0.004*make + 0.004*know + 0.003*ll')\n",
      "Topic: 30\n",
      "(30, u'0.014*headline + 0.011*ralph + 0.010*explosive + 0.007*irritated + 0.006*manicure')\n",
      "Topic: 31\n",
      "(31, u'0.004*news + 0.003*people + 0.003*new + 0.002*years + 0.002*world')\n",
      "Topic: 32\n",
      "(32, u'0.009*humour + 0.008*admire + 0.006*heroin + 0.005*diuretic + 0.005*branch')\n",
      "Topic: 33\n",
      "(33, u'0.003*people + 0.003*food + 0.002*best + 0.002*healthy + 0.002*foods')\n",
      "Topic: 34\n",
      "(34, u'0.007*recipes + 0.006*ideas + 0.005*healthy + 0.005*join + 0.005*member')\n",
      "Topic: 35\n",
      "(35, u'0.008*com + 0.007*download + 0.005*android + 0.005*iphone + 0.005*new')\n",
      "Topic: 36\n",
      "(36, u'0.007*tokyo + 0.006*opposition + 0.006*dudes + 0.005*japanese + 0.005*wordpress')\n",
      "Topic: 37\n",
      "(37, u'0.009*sanchez + 0.009*prized + 0.008*jets + 0.007*zap + 0.007*knotted')\n",
      "Topic: 38\n",
      "(38, u'0.009*19th + 0.007*tennessee + 0.007*mega + 0.006*arena + 0.005*fulfill')\n",
      "Topic: 39\n",
      "(39, u'0.003*just + 0.003*time + 0.003*use + 0.003*like + 0.002*think')\n",
      "Topic: 40\n",
      "(40, u'0.009*nba + 0.009*basketball + 0.006*court + 0.005*yankees + 0.005*district')\n",
      "Topic: 41\n",
      "(41, u'0.005*bailey + 0.005*packers + 0.004*criminals + 0.004*enlarge + 0.004*recipe')\n",
      "Topic: 42\n",
      "(42, u'0.004*like + 0.004*food + 0.004*meat + 0.003*salad + 0.003*garlic')\n",
      "Topic: 43\n",
      "(43, u'0.004*toronto + 0.004*el + 0.004*eats + 0.004*collision + 0.004*alert')\n",
      "Topic: 44\n",
      "(44, u'0.008*33 + 0.008*71 + 0.007*39 + 0.007*74 + 0.007*86')\n",
      "Topic: 45\n",
      "(45, u'0.004*like + 0.004*easy + 0.004*make + 0.004*day + 0.003*use')\n",
      "Topic: 46\n",
      "(46, u'0.013*elementary + 0.013*olympics + 0.010*sport + 0.008*und + 0.008*olympic')\n",
      "Topic: 47\n",
      "(47, u'0.012*garlic1 + 0.010*slideshow + 0.010*amusing + 0.007*jfk + 0.007*hotties')\n",
      "Topic: 48\n",
      "(48, u'0.007*div + 0.006*harrison + 0.005*ford + 0.005*pga + 0.005*ap')\n",
      "Topic: 49\n",
      "(49, u'0.008*investigated + 0.007*halo + 0.007*laboratories + 0.006*nintendo + 0.006*stations')\n",
      "Topic: 50\n",
      "(50, u'0.007*loop + 0.006*decker + 0.005*clearer + 0.005*extraordinarily + 0.004*brooklyn')\n",
      "Topic: 51\n",
      "(51, u'0.012*recruiting + 0.009*portrait + 0.007*buckwheat + 0.006*mats + 0.006*questioned')\n",
      "Topic: 52\n",
      "(52, u'0.007*sugar + 0.007*vanilla + 0.006*bake + 0.006*beat + 0.006*mixer')\n",
      "Topic: 53\n",
      "(53, u'0.006*minutes + 0.006*recipe + 0.005*cup + 0.005*add + 0.005*make')\n",
      "Topic: 54\n",
      "(54, u'0.004*year + 0.002*laptop + 0.002*just + 0.002*world + 0.002*claims')\n",
      "Topic: 55\n",
      "(55, u'0.004*time + 0.004*make + 0.003*like + 0.003*just + 0.003*food')\n",
      "Topic: 56\n",
      "(56, u'0.012*firing + 0.008*gun + 0.008*saltine + 0.008*lied + 0.007*reed')\n",
      "Topic: 57\n",
      "(57, u'0.009*policy + 0.007*permission + 0.007*privacy + 0.006*constitutes + 0.005*make')\n",
      "Topic: 58\n",
      "(58, u'0.006*arsenal + 0.004*alerts + 0.004*peppercorns + 0.004*specialist + 0.004*prunes')\n",
      "Topic: 59\n",
      "(59, u'0.009*foods + 0.008*health + 0.007*diet + 0.007*nutrition + 0.006*body')\n",
      "Topic: 60\n",
      "(60, u'0.015*mug + 0.011*macadamia + 0.008*acorn + 0.008*objects + 0.007*bikini')\n",
      "Topic: 61\n",
      "(61, u'0.006*indication + 0.005*represented + 0.005*calf + 0.005*originates + 0.005*200c')\n",
      "Topic: 62\n",
      "(62, u'0.016*sports + 0.013*rankings + 0.010*athletes + 0.010*multiple + 0.010*swagger')\n",
      "Topic: 63\n",
      "(63, u'0.005*sided + 0.004*crushing + 0.004*broiled + 0.003*ricotta + 0.003*cored')\n",
      "Topic: 64\n",
      "(64, u'0.025*photo + 0.021*models + 0.020*sports + 0.019*si + 0.017*illustrated')\n",
      "Topic: 65\n",
      "(65, u'0.003*time + 0.003*food + 0.003*like + 0.003*good + 0.002*just')\n",
      "Topic: 66\n",
      "(66, u'0.003*time + 0.003*make + 0.003*like + 0.003*way + 0.002*virus')\n",
      "Topic: 67\n",
      "(67, u'0.006*murder + 0.006*kong + 0.006*seats + 0.005*hoops + 0.005*crammed')\n",
      "Topic: 68\n",
      "(68, u'0.004*help + 0.002*world + 0.002*new + 0.002*events + 0.002*like')\n",
      "Topic: 69\n",
      "(69, u'0.008*shouting + 0.007*oklahoma + 0.006*visualization + 0.006*aliens + 0.006*fierce')\n",
      "Topic: 70\n",
      "(70, u'0.010*browse + 0.009*martha + 0.008*recipes + 0.008*food + 0.008*hundreds')\n",
      "Topic: 71\n",
      "(71, u'0.016*licensed + 0.010*boing + 0.010*commercial + 0.010*permitting + 0.009*driveway')\n",
      "Topic: 72\n",
      "(72, u'0.013*devices + 0.012*phones + 0.012*technology + 0.012*tech + 0.011*gadgets')\n",
      "Topic: 73\n",
      "(73, u'0.004*cathy + 0.004*clips + 0.004*returning + 0.003*like + 0.003*sleepy')\n",
      "Topic: 74\n",
      "(74, u'0.009*anna + 0.006*nfl + 0.006*sports + 0.005*hockey + 0.005*ranking')\n",
      "Topic: 75\n",
      "(75, u'0.006*pharmacy + 0.003*las + 0.003*idaho + 0.003*myriad + 0.003*del')\n",
      "Topic: 76\n",
      "(76, u'0.003*use + 0.003*need + 0.003*acid + 0.002*high + 0.002*time')\n",
      "Topic: 77\n",
      "(77, u'0.010*heath + 0.010*crepes + 0.009*ebook + 0.008*murray + 0.007*madness')\n",
      "Topic: 78\n",
      "(78, u'0.003*aids + 0.003*people + 0.003*like + 0.003*information + 0.003*telephone')\n",
      "Topic: 79\n",
      "(79, u'0.012*pakistan + 0.011*alaska + 0.011*israel + 0.010*flown + 0.008*iowa')\n",
      "Topic: 80\n",
      "(80, u'0.017*technology + 0.008*concept + 0.007*user + 0.007*design + 0.007*technologies')\n",
      "Topic: 81\n",
      "(81, u'0.003*time + 0.002*like + 0.002*people + 0.002*said + 0.002*work')\n",
      "Topic: 82\n",
      "(82, u'0.012*style + 0.011*fashion + 0.009*dress + 0.007*wear + 0.006*look')\n",
      "Topic: 83\n",
      "(83, u'0.009*upload + 0.006*gq + 0.006*priorities + 0.005*qualified + 0.005*baskets')\n",
      "Topic: 84\n",
      "(84, u'0.008*unmanned + 0.008*hovering + 0.007*landing + 0.007*locals + 0.007*company')\n",
      "Topic: 85\n",
      "(85, u'0.005*celebrations + 0.004*surfer + 0.004*karen + 0.003*macaroons + 0.003*ina')\n",
      "Topic: 86\n",
      "(86, u'0.004*games + 0.004*masterpiece + 0.004*time + 0.003*scores + 0.003*fantasy')\n",
      "Topic: 87\n",
      "(87, u'0.007*width + 0.007*gif + 0.006*1px + 0.006*forums + 0.006*display')\n",
      "Topic: 88\n",
      "(88, u'0.008*visual + 0.007*fashion + 0.007*cinch + 0.006*artist + 0.006*custom')\n",
      "Topic: 89\n",
      "(89, u'0.008*wallpapers + 0.006*betty + 0.006*pep + 0.005*spit + 0.005*crocker')\n",
      "Topic: 90\n",
      "(90, u'0.004*fixing + 0.003*bf + 0.003*12 + 0.003*ronaldo + 0.003*uri')\n",
      "Topic: 91\n",
      "(91, u'0.006*recipes + 0.004*recipe + 0.004*chicken + 0.004*ingredients + 0.004*good')\n",
      "Topic: 92\n",
      "(92, u'0.004*just + 0.004*use + 0.003*like + 0.003*people + 0.003*make')\n",
      "Topic: 93\n",
      "(93, u'0.004*health + 0.004*people + 0.003*like + 0.003*time + 0.003*body')\n",
      "Topic: 94\n",
      "(94, u'0.008*abc + 0.008*logos + 0.007*simplify + 0.007*martini + 0.007*provincial')\n",
      "Topic: 95\n",
      "(95, u'0.009*glamour + 0.008*8220 + 0.008*8221 + 0.007*marzipan + 0.007*snickerdoodles')\n",
      "Topic: 96\n",
      "(96, u'0.005*haha + 0.004*smores + 0.004*modifications + 0.003*disk + 0.003*dorie')\n",
      "Topic: 97\n",
      "(97, u'0.006*logged + 0.006*privately + 0.006*tutorials + 0.005*sync + 0.005*orbit')\n",
      "Topic: 98\n",
      "(98, u'0.008*fields + 0.007*code + 0.006*del + 0.005*use + 0.005*title')\n",
      "Topic: 99\n",
      "(99, u'0.007*shifts + 0.007*bagels + 0.007*etsy + 0.006*hamilton + 0.006*dyes')\n"
     ]
    }
   ],
   "source": [
    "num_topics = 100\n",
    "n_words_per_topic = 5\n",
    "for ti, topic in enumerate(lda_model.show_topics(num_topics = num_topics,num_words = n_words_per_topic,formatted = True)):\n",
    "    print 'Topic: %d' % ti\n",
    "    print topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = data.body.dropna().map(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "u\"word 'sagi' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-71f6dfb2afd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Min_count argument only keeps those words that occur at least 5 times in a corpus. Other words are probably garbage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sagi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/sagi/miniconda2/lib/python2.7/site-packages/gensim/models/word2vec.pyc\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: u\"word 'sagi' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#Min_count argument only keeps those words that occur at least 5 times in a corpus. Other words are probably garbage.\n",
    "model = Word2Vec(text,size = 100,window = 5,min_count = 5, workers = 4)\n",
    "model.most_similar(positive = [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the tweet data\n",
    "filename = '../../assets/dataset/captured-tweets.txt'\n",
    "tweets = []\n",
    "for tweet in codecs.open(filename, 'r', encoding=\"utf-8\"):\n",
    "    tweets.append(tweet)\n",
    "# Setting up spacy\n",
    "nlp_toolkit = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1a\n",
    "\n",
    "Write a function that can take a take a sentence parsed by `spacy` and identify if it mentions a company named 'Google'. Remember, `spacy` can find entities and codes them as `ORG` if they are a company. Look at the slides for class 13 if you need a hint:\n",
    "\n",
    "### Bonus (1b)\n",
    "\n",
    "Parameterize the company name so that the function works for any company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_sentence = nlp_toolkit('Google drives a car that runs on a wheel')\n",
    "type(parsed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parsing in Spacy is of type Document\n",
    "\n",
    "def contains_google(parsed_sentence):\n",
    "    \n",
    "    for word in parsed_sentence:\n",
    "        print word\n",
    "        if (word.ent_type_ == 'ORG') and (str(word) == 'Google'):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Google\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_google(parsed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for word in parsed_sentence:\n",
    "    print str(word) == 'Google'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def mentions_company(parsed):\n",
    "    # Return True if the sentence contains an organization and that organization is Google\n",
    "#   for entity in parsed.ents:\n",
    "#        print entity\n",
    "#        if entity == 'Google':\n",
    "#            return True\n",
    "#    return False\n",
    "    #return False\n",
    "\n",
    "# 1b\n",
    "\n",
    "def mentions_company(parsed, company='Google'):\n",
    "    for word in parsed:\n",
    "        if (word.ent_type_ == 'ORG') and word.text == company:\n",
    "            return True\n",
    "    return False    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_company(parsed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1c\n",
    "\n",
    "Write a function that can take a sentence parsed by `spacy` \n",
    "and return the verbs of the sentence (preferably lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_actions(parsed):\n",
    "    actions = []\n",
    "    \n",
    "    for word in parsed:\n",
    "        if word.pos_ == 'VERB':\n",
    "            actions.append(word.text)\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'drives', u'runs']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actions(parsed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'drives' in get_actions(parsed_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1d\n",
    "For each tweet, parse it using spacy and print it out if the tweet has 'release' or 'announce' as a verb. You'll need to use your `mentions_company` and `get_actions` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    \n",
    "    verbs = get_actions(parsed)\n",
    "    \n",
    "    if ('release' in verbs or 'announce' in verbs) and mentions_company(parsed):\n",
    "        print tweet\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 1e\n",
    "Write a function that identifies countries - HINT: the entity label for countries is GPE (or GeoPolitical Entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mentions_country(parsed, country):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1f\n",
    "\n",
    "Re-run (d) to find country tweets that discuss 'Iran' announcing or releasing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Build a `word2vec` model of the tweets we have collected using `gensim`.\n",
    "\n",
    "### Exercise 2a:\n",
    "First take the collection of tweets and tokenize them using spacy.\n",
    "\n",
    "* Think about how this should be done. \n",
    "* Should you only use upper-case or lower-case? \n",
    "* Should you remove punctuations or symbols? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_split = [[x.text if x.pos != spacy.parts_of_speech.VERB else x.lemma_ \n",
    "                for x in nlp_toolkit(t)] for t in tweets]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2b:\n",
    "Build a `word2vec` model.\n",
    "Test the window size as well - this is how many surrounding words need to be used to model a word. What do you think is appropriate for Twitter? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(text_split, size=100, window=4, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2c:\n",
    "Test your word2vec model with a few similarity functions. \n",
    "* Find words similar to 'Syria'.\n",
    "* Find words similar to 'war'.\n",
    "* Find words similar to \"Iran\".\n",
    "* Find words similar to 'Verizon'. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(positive=['Syria'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2d\n",
    "\n",
    "Adjust the choices / parameters in (b) and (c) as necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Filter tweets to those that mention 'Iran' or similar entities and 'war' or similar entities.\n",
    "* Do this using just spacy.\n",
    "* Do this using word2vec similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using spacy\n",
    "for tweet in tweets:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using word2vec similarity scores\n",
    "for tweet in tweets[:200]:\n",
    "    parsed = nlp_toolkit(tweet)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
